SVM Intuition

Support Vector Machines (SVMs) are a machine learning algorithm used for classification and regression tasks.
It tries to find the best boundary known as a hyperplane that separates different classes in the data.
It is useful when you want to do binary classification such as spam vs not spam or cat vs dog.

Key Concepts:
    - Hyperplane
        A decision boundary separating different classes in feature space and is represented by the equation
        wx + b = 0 in linear classification.
    - Support Vectors
        The closest data points to the hyperplane, crucial for determining the hyperplane and margin in SVM.
    - Margin
        The distance between the hyperplane and the support vectors. SVM aims to maximize this margin for better
        classification performance.
    - Kernal
        A function that maps data to a higher-dimensional space enabling SVM to handle non-linearly separable data.
    - Hard Margin
        A maximum-margin hyperplane that perfectly separates the data without misclassifications.
    - Soft Margin
        Allows some misclassifications by introducing slack variables, balancing margin maximization and misclassification
        penalties when data is not perfectly separable.
    - C
        A regularization term balancing margin maximization and misclassification penalties. A higher C value forces stricter SVM.
    - Hinge Loss
        A loss function penalizing misclassified points or margin violations and is combined with regularization in SVM.
    - Dual Problem
        Involves solving for Lagrange multipliers associated with support vectors, facilitating the kernal trick and efficient
        computation.

The key idea behind the SVM algorithm is to find the hyperplane that best separates two classes by maximizing the margin between them.
This margin is the distance from the hyperplane to the nearest data points (support vecotrs) on each side.

The best hyperplane is also known as the "hard margin", the one that maximizes the distance between the hyperplane and the nearest
data points from both classes. This ensures a clear separation between the classes.

SVM classifies data points by determining which side of the hyperplane they fall on. If a data point is on one side of the hyperplane, 
it is classified as one class, and if it is on the other side, it is classified as the other class.
